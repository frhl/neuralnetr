% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Linear.R
\name{Linear}
\alias{Linear}
\title{Linear modules}
\description{
A linear moduls implements a linear transformation:

\deqn{..tba}

specified by a weight matrix w and a bias vector w0. Each linear module has
a forward method that takes in a batch of activations A (from the previous layer)
and returns a batch of pre-activations Z.

Each linear module has a backward method that takes in dLdZ and
returns dLdA. This module also computes and stores dLdW and dLdW0,
the gradients with respect to the weights.
}
\seealso{
Other architecture: 
\code{\link{Sequential}}
}
\concept{architecture}
\section{Super class}{
\code{\link[neuralnetr:ClassModule]{neuralnetr::ClassModule}} -> \code{Module}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{Linear$new()}}
\item \href{#method-forward}{\code{Linear$forward()}}
\item \href{#method-backward}{\code{Linear$backward()}}
\item \href{#method-sgd_step}{\code{Linear$sgd_step()}}
\item \href{#method-clone}{\code{Linear$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\subsection{Method \code{new()}}{
initialize the weights.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Linear$new(m, n)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{m}}{the m dimension of the module.}

\item{\code{n}}{the n dimension of the module.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-forward"></a>}}
\subsection{Method \code{forward()}}{
do one step forward.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Linear$forward(A)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{A}}{input activation (m x b)}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Z pre-activation (n x b)
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-backward"></a>}}
\subsection{Method \code{backward()}}{
do one gradient step backward
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Linear$backward(dLdZ)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{dLdZ}}{the derivative of the loss with
respect to Z (n x b)}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
dLdA (m x b)
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-sgd_step"></a>}}
\subsection{Method \code{sgd_step()}}{
update the weights using
stochastic gradient descent
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Linear$sgd_step(lrate)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{lrate}}{learning rate}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{Linear$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
